üß† Overview

This experiment demonstrates how to build a lightweight research Lakehouse architecture using DuckDB and Parquet files. The workflow simulates ingesting data, cleaning it, storing it in Parquet format, and querying it efficiently   all inside a single notebook environment. The goal is to illustrate how researchers can build reproducible and scalable analytical pipelines without relying on heavy distributed systems.

‚úèÔ∏è Objective

‚Ä¢	Create a minimal Lakehouse using DuckDB
‚Ä¢	Store clean analytical data in Parquet format
‚Ä¢	Execute analytical queries using SQL over local Parquet datasets
‚Ä¢	Demonstrate a reproducible research-grade data pipeline

üìò Results

‚Ä¢	Raw dataset successfully loaded and explored
‚Ä¢	Clean dataset generated by removing missing values and duplicates
‚Ä¢	Data stored efficiently as Parquet
‚Ä¢	Analytical SQL queries executed on Parquet files with fast performance
‚Ä¢	The experiment achieved a full local Lakehouse pipeline suitable for research environments

üìó Notes

‚Ä¢	DuckDB provides a serverless, embedded OLAP engine ideal for rapid prototyping
‚Ä¢	Parquet format ensures compression and columnar access, improving performance and storage efficiency
‚Ä¢	The pipeline can be easily extended to cloud object storage (e.g., S3, GCS, Azure Blob)
‚Ä¢	Suitable for academic workflows, government research units, and data labs building reproducible analytical pipelines

